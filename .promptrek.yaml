# PrompTrek Self-Configuration
# This configuration optimizes AI assistants for working on the PrompTrek project itself

schema_version: "1.0.0"

metadata:
  title: "PrompTrek AI Editor Prompts"
  description: "AI assistant configuration for developing PrompTrek - Universal AI Editor prompt management tool"
  version: "1.0.0"
  author: "PrompTrek Development Team"
  created: "2024-01-01"
  updated: "2024-01-15"
  tags: ["python", "cli", "ai-tools", "prompt-management", "adapter-pattern", "click"]

targets:
  - copilot
  - cursor
  - continue
  - claude
  - cline
  - codeium

context:
  project_type: "cli_tool"
  technologies:
    - "python"
    - "click"
    - "pyyaml"
    - "pydantic"
    - "jinja2"
    - "pytest"
    - "black"
    - "mypy"
    - "isort"
  description: |
    PrompTrek is a universal AI editor prompt management tool written in Python.
    It uses the adapter pattern to support multiple AI editors (Cursor, Continue, 
    GitHub Copilot, Cline, etc.) and converts Universal Prompt Format (UPF) files
    into editor-specific configurations. Built with Click CLI framework and 
    comprehensive testing infrastructure.
  repository_url: "https://github.com/flamingquaks/promptrek"
  documentation_url: "https://github.com/flamingquaks/promptrek/tree/main/docs"

instructions:
  general:
    - "When updating documentation, make sure to update the documents in the docs/ directory. Also review the gh-pages/ directory for updating the GitHub Pages site."
  
  code_style:
    - "Format code with Black (line length 88)"
    - "Sort imports with isort using Black profile"
    - "Use type annotations for all functions and methods"
    - "Prefer pathlib.Path over os.path for file operations"
    - "Use click.echo() instead of print() for CLI output"
    - "Follow naming conventions: snake_case for functions/variables, PascalCase for classes"
    - "Use f-strings for string formatting when possible"
  
  architecture:
    - "Follow the adapter pattern: inherit from EditorAdapter base class"
    - "Keep core models in src/promptrek/core/models.py using Pydantic"
    - "Implement CLI commands in src/promptrek/cli/commands/ directory"
    - "Use dependency injection and avoid tight coupling between components"
    - "Separate concerns: parsing, validation, generation, and CLI interface"
    - "Each adapter should handle one specific AI editor type"
    - "Use registry pattern for adapter discovery and management"
  
  testing:
    - "Write unit tests for all public methods and functions"
    - "Use pytest fixtures for common test setup"
    - "Mock external dependencies and file operations in tests"
    - "Test both success and failure scenarios"
    - "Maintain 80%+ code coverage"
    - "Write integration tests for CLI commands"
    - "Use parametrized tests for testing multiple adapter types"
  
  security:
    - "Validate all user inputs using Pydantic models"
    - "Sanitize file paths to prevent directory traversal"
    - "Use safe YAML loading (yaml.safe_load)"
    - "Validate file permissions before writing files"
  
  performance:
    - "Use lazy loading for adapter imports"
    - "Cache compiled regex patterns"
    - "Minimize file I/O operations in loops"
    - "Use generators for large data processing"

examples:
  adapter_implementation: |
    ```python
    """Example adapter implementation following PrompTrek patterns."""
    
    from pathlib import Path
    from typing import List, Optional, Dict, Any
    from ..core.models import UniversalPrompt
    from ..core.exceptions import ValidationError
    from .base import EditorAdapter
    
    class ExampleAdapter(EditorAdapter):
        """Adapter for Example AI Editor."""
        
        def __init__(self):
            super().__init__(
                name="example",
                description="Example AI Editor with project-level configuration",
                file_patterns=["example.config.yaml", ".example/settings.json"]
            )
        
        def generate(
            self, 
            prompt: UniversalPrompt,
            output_dir: Path,
            dry_run: bool = False,
            verbose: bool = False,
            variables: Optional[Dict[str, Any]] = None
        ) -> List[Path]:
            """Generate editor-specific configuration files."""
            # Process variables and conditionals
            processed_prompt = self.substitute_variables(prompt, variables)
            conditional_content = self.process_conditionals(processed_prompt, variables)
            
            # Generate configuration content
            config_content = self._build_config(processed_prompt, conditional_content)
            
            # Write files
            config_path = output_dir / "example.config.yaml"
            if not dry_run:
                config_path.parent.mkdir(parents=True, exist_ok=True)
                config_path.write_text(config_content)
                
            return [config_path]
        
        def validate(self, prompt: UniversalPrompt) -> List[ValidationError]:
            """Validate prompt compatibility with this editor."""
            errors = []
            
            # Example validation: check for required instructions
            if not prompt.instructions or not prompt.instructions.general:
                errors.append(ValidationError(
                    "Example editor requires general instructions"
                ))
                
            return errors
        
        def supports_variables(self) -> bool:
            return True
            
        def supports_conditionals(self) -> bool:
            return True
    ```
  
  pydantic_model: |
    ```python
    """Example Pydantic model following PrompTrek patterns."""
    
    from datetime import datetime
    from typing import List, Optional
    from pydantic import BaseModel, Field, field_validator
    
    class EditorConfig(BaseModel):
        """Configuration for a specific AI editor."""
        
        name: str = Field(..., description="Editor name")
        version: str = Field(..., description="Configuration version")
        instructions: List[str] = Field(default_factory=list)
        enabled_features: Optional[List[str]] = Field(default=None)
        created_at: str = Field(..., description="ISO 8601 timestamp")
        
        @field_validator("created_at")
        @classmethod
        def validate_timestamp(cls, v: str) -> str:
            """Validate ISO 8601 timestamp format."""
            try:
                datetime.fromisoformat(v.replace('Z', '+00:00'))
            except ValueError:
                raise ValueError("Invalid timestamp format")
            return v
        
        @field_validator("name")
        @classmethod
        def validate_name(cls, v: str) -> str:
            """Validate editor name format."""
            if not v.isalnum():
                raise ValueError("Editor name must be alphanumeric")
            return v.lower()
    ```
  
  cli_command: |
    ```python
    """Example CLI command following PrompTrek patterns."""
    
    import click
    from pathlib import Path
    from ..core.exceptions import PrompTrekError
    from ..core.parser import PromptParser
    
    @click.command()
    @click.argument("file", type=click.Path(exists=True, path_type=Path))
    @click.option("--output", "-o", type=click.Path(path_type=Path), help="Output directory")
    @click.option("--dry-run", is_flag=True, help="Show what would be generated")
    @click.pass_context
    def example_command(ctx: click.Context, file: Path, output: Path, dry_run: bool) -> None:
        """Example CLI command with proper error handling."""
        verbose = ctx.obj.get("verbose", False)
        
        try:
            # Parse the prompt file
            parser = PromptParser()
            prompt = parser.parse_file(file)
            
            # Perform operation
            if verbose:
                click.echo(f"Processing {file}")
            
            # Success output
            click.echo("✅ Command completed successfully")
            
        except PrompTrekError as e:
            click.echo(f"❌ Error: {e}", err=True)
            if verbose:
                raise
            ctx.exit(1)
        except Exception as e:
            click.echo(f"❌ Unexpected error: {e}", err=True)
            if verbose:
                raise
            ctx.exit(1)
    ```
  
  test_pattern: |
    ```python
    """Example test following PrompTrek testing patterns."""
    
    import pytest
    from pathlib import Path
    from unittest.mock import Mock, patch
    from promptrek.adapters.example import ExampleAdapter
    from promptrek.core.models import UniversalPrompt, PromptMetadata
    
    class TestExampleAdapter:
        """Test suite for ExampleAdapter."""
        
        @pytest.fixture
        def adapter(self):
            """Create adapter instance."""
            return ExampleAdapter()
        
        @pytest.fixture
        def sample_prompt(self):
            """Create sample universal prompt."""
            return UniversalPrompt(
                schema_version="1.0.0",
                metadata=PromptMetadata(
                    title="Test",
                    description="Test prompt",
                    version="1.0.0",
                    author="test@example.com",
                    created="2024-01-01",
                    updated="2024-01-01"
                ),
                targets=["example"]
            )
        
        def test_generate_creates_config_file(self, adapter, sample_prompt, tmp_path):
            """Test that generate creates expected configuration file."""
            # Act
            result = adapter.generate(sample_prompt, tmp_path, dry_run=False)
            
            # Assert
            assert len(result) == 1
            assert result[0].name == "example.config.yaml"
            assert result[0].exists()
        
        def test_generate_dry_run_no_files(self, adapter, sample_prompt, tmp_path):
            """Test that dry run doesn't create files."""
            # Act
            result = adapter.generate(sample_prompt, tmp_path, dry_run=True)
            
            # Assert
            assert len(result) == 1
            assert not result[0].exists()
        
        def test_validate_missing_instructions(self, adapter):
            """Test validation fails for missing instructions."""
            # Arrange
            prompt = UniversalPrompt(
                schema_version="1.0.0",
                metadata=PromptMetadata(
                    title="Test", description="Test", version="1.0.0",
                    author="test@example.com", created="2024-01-01", updated="2024-01-01"
                ),
                targets=["example"],
                instructions=None
            )
            
            # Act
            errors = adapter.validate(prompt)
            
            # Assert
            assert len(errors) > 0
            assert "general instructions" in str(errors[0])
    ```

variables:
  PROJECT_NAME: "PrompTrek"
  REPO_URL: "https://github.com/flamingquaks/promptrek"
  PYTHON_VERSION: "3.8+"
  CLI_FRAMEWORK: "Click"
  VALIDATION_FRAMEWORK: "Pydantic"
  TEST_FRAMEWORK: "pytest"

